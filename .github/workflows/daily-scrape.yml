name: Daily Domain Scrape

on:
  schedule:
    # Run every day at 9 AM UTC
    - cron: '0 9 * * *'

  # Allow manual trigger
  workflow_dispatch:

jobs:
  scrape-domains:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: domain_finder
          POSTGRES_PASSWORD: ${{ secrets.DB_PASSWORD || 'changeme' }}
          POSTGRES_DB: domain_finder
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        working-directory: ./backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run daily scrape job
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://domain_finder:${{ secrets.DB_PASSWORD || 'changeme' }}@localhost:5432/domain_finder
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
          SENDGRID_API_KEY: ${{ secrets.SENDGRID_API_KEY }}
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
          WHOIS_JSON_API_KEY: ${{ secrets.WHOIS_JSON_API_KEY }}
          USE_SAMPLE_DATA: 'true'
        run: |
          python -c "
          import os
          import sys
          import logging
          from database import SessionLocal
          from tasks.scheduled_tasks import TaskScheduler

          logging.basicConfig(level=logging.INFO)
          logger = logging.getLogger(__name__)

          logger.info('Starting GitHub Actions daily scrape job...')

          try:
              db_session = SessionLocal()
              TaskScheduler.daily_scrape_job(db_session)
              logger.info('Scrape job completed successfully')
          except Exception as e:
              logger.error(f'Scrape job failed: {e}', exc_info=True)
              sys.exit(1)
          "

      - name: Send Slack notification on failure
        if: failure()
        uses: slackapi/slack-github-action@v1
        with:
          payload: |
            {
              "text": "❌ Domain Finder Pro: Daily scrape job failed",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Domain Finder Pro - Daily Scrape Failed*\n${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                  }
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}

      - name: Send Slack notification on success
        if: success()
        uses: slackapi/slack-github-action@v1
        with:
          payload: |
            {
              "text": "✅ Domain Finder Pro: Daily scrape job completed",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Domain Finder Pro - Daily Scrape Completed*\nSuccessfully scraped and analyzed domains"
                  }
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
